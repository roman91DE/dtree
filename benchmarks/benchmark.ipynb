{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.06s\n"
     ]
    }
   ],
   "source": [
    "def build_dtree():\n",
    "    subprocess.run([\"cargo\", \"build\", \"--release\"])\n",
    "    \n",
    "build_dtree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command: list[str]) -> tuple[float, int, str]:\n",
    "    \"\"\"\n",
    "    Runs a command and captures execution time, peak memory usage, and output.\n",
    "    \n",
    "    Args:\n",
    "        command (list[str]): Command to execute as a list of arguments.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, int, str]: Execution time (seconds), peak memory usage (KB), and output.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    process = subprocess.Popen(\n",
    "        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True\n",
    "    )\n",
    "    stdout, stderr = process.communicate()\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Parse memory usage from /usr/bin/time format\n",
    "    memory_usage = 0\n",
    "    if stderr:\n",
    "        for line in stderr.splitlines():\n",
    "            if \"maxresident\" in line:\n",
    "                memory_usage = int(line.split()[-1])\n",
    "\n",
    "    return execution_time, memory_usage, stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_programs(test_dirs: list[str], my_program: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Benchmarks the user's program and the `tree` command across multiple directories\n",
    "    with various argument combinations.\n",
    "\n",
    "    Args:\n",
    "        test_dirs (list[str]): List of directories to benchmark on.\n",
    "        my_program (str): Path to the user's tree-like program.\n",
    "        output_file (str): File path to save the benchmark results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Define argument combinations to test\n",
    "    my_program_args = [\n",
    "        [],  # Default arguments\n",
    "        [\"-L\", \"2\"], \n",
    "        # [\"-L\", \"5\"],  # Limit depth\n",
    "        [\"-a\"],  # Show hidden files\n",
    "        # [\"-f\"],  # Full paths\n",
    "        [\"-L\", \"3\", \"-a\", \"-f\"],  # Combination of all flags\n",
    "    ]\n",
    "\n",
    "    tree_program_args = [\n",
    "        [],  # Default arguments\n",
    "        [\"-L\", \"2\"],  # Limit depth\n",
    "        # [\"-L\", \"5\"],  # Limit depth\n",
    "        [\"-a\"],  # Show hidden files\n",
    "        # [\"-f\"],  # Full paths\n",
    "        [\"-L\", \"3\", \"-a\", \"-f\"],  # Combination of all flags\n",
    "    ]\n",
    "\n",
    "    for directory in test_dirs:\n",
    "        print(f\"Benchmarking on: {directory}\")\n",
    "\n",
    "        for my_args, tree_args in zip(my_program_args, tree_program_args):\n",
    "            # Run my_program with arguments\n",
    "            my_command = [my_program] + my_args + [directory]\n",
    "            my_time, my_memory, my_output = run_command(my_command)\n",
    "\n",
    "            # Run tree command with arguments\n",
    "            tree_command = [\"tree\"] + tree_args + [directory]\n",
    "            tree_time, tree_memory, tree_output = run_command(tree_command)\n",
    "\n",
    "            # Validate outputs\n",
    "            output_match = \"Match\" if my_output == tree_output else \"Mismatch\"\n",
    "\n",
    "            # Save results\n",
    "            results.append({\n",
    "                \"Directory\": directory,\n",
    "                \"Program\": \"dtree\",\n",
    "                \"Arguments\": \" \".join(my_args),\n",
    "                \"Time(s)\": my_time,\n",
    "                \"Memory(KB)\": my_memory,\n",
    "                \"OutputMatch\": output_match,\n",
    "            })\n",
    "\n",
    "            results.append({\n",
    "                \"Directory\": directory,\n",
    "                \"Program\": \"tree\",\n",
    "                \"Arguments\": \" \".join(tree_args),\n",
    "                \"Time(s)\": tree_time,\n",
    "                \"Memory(KB)\": tree_memory,\n",
    "                \"OutputMatch\": \"-\",\n",
    "            })\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Benchmark results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on: /Users/roman/Repos/dtree\n",
      "Benchmarking on: /Users/roman\n",
      "Benchmark results saved to /Users/roman/Repos/dtree/benchmarks/results.csv\n"
     ]
    }
   ],
   "source": [
    "# List of directories to benchmark on\n",
    "test_directories = [\n",
    "    str(Path.cwd().parent),\n",
    "    str(Path().home())\n",
    "    ]\n",
    "\n",
    "# Path to the user's tree-like program\n",
    "user_program_path = str(Path().cwd().parent / \"target/release/dtree\")\n",
    "\n",
    "# Output CSV file\n",
    "output_csv = str(Path.cwd() / \"results.csv\")\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_programs(test_directories, user_program_path, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
